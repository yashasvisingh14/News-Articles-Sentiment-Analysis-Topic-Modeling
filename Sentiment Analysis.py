# -*- coding: utf-8 -*-
"""Projec_ Code_G6_Part1 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P35ZxH8yaZoqMGbtxWZp9uKTY15e8Tpv

## **NEWS ARTICLES SENTIMENT ANALYSIS**
"""

import requests, json
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from google.colab import files
import pandas as pd
import numpy as np
import re
import seaborn as sns
import matplotlib.pyplot as plt
import collections
from wordcloud import WordCloud, STOPWORDS

uploaded = files.upload()

uploaded = files.upload()

uploaded = files.upload()

breitbart = []
with open("finished_data_breitbart.json","r",encoding='utf-8') as fd:
    for line in fd.readlines():
        breitbart.append(json.loads(line.strip()))
fox = []
with open("finished_data_fox.json","r",encoding='utf-8') as fd:
    for line in fd.readlines():
        fox.append(json.loads(line.strip()))
newsmax = []
with open("finished_data_newsmax.json","r",encoding='utf-8') as fd:
    for line in fd.readlines():
        newsmax.append(json.loads(line.strip()))

breitbart = pd.DataFrame(breitbart)
fox = pd.DataFrame(fox)
newsmax = pd.DataFrame(newsmax)

breitbart = breitbart.replace('\n','', regex=True)
fox = fox.replace('\n','', regex=True)
newsmax = newsmax.replace('\n','', regex=True)

# converting dates
breitbart['date'] = pd.to_datetime(breitbart['date'])
fox['date'] = pd.to_datetime(fox['date'])
newsmax['date'] = pd.to_datetime(newsmax['date'])

breitbart

fox

newsmax

nltk.download('wordnet')
nltk.download('omw-1.4')
wnl = nltk.WordNetLemmatizer()
type(wnl)

textb = " ".join(d for d in breitbart.text.astype(str))

textf = " ".join(d for d in fox.text.astype(str))

textn = " ".join(d for d in newsmax.text.astype(str))

title = " ".join(d for d in breitbart.title.astype(str))

stopwords = nltk.corpus.stopwords.words('english')
new_words=('say','u', 'said', 'says')
for i in new_words:
    stopwords.append(i)
print(stopwords)

mask = (breitbart['date'] > '2022-02-01') & (breitbart['date'] <= '2022-02-14') # select a period
df1 = breitbart.loc[mask]
title01 = " ".join(d for d in df1.title.astype(str))

stop_pat = r'\b(?:{})\b'.format('|'.join(stopwords))

def ReturnCleanText(text):
         text = text.lower()
         text = re.sub(r"\W+|_", ' ', text)
         return re.sub(stop_pat, '', text)

df1.title = df1.title.apply(ReturnCleanText)

df1tokens=nltk.word_tokenize("/n".join(df1.title))
df1lemma = [wnl.lemmatize(t) for t in df1tokens]

count1 = collections.Counter(df1lemma)
word_freq1 = pd.DataFrame(count1.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)

mask2 = (fox['date'] > '2022-02-01') & (fox['date'] <= '2022-02-14')
df2 = fox.loc[mask2]
title02 = " ".join(d for d in df2.title.astype(str))

stop_pat = r'\b(?:{})\b'.format('|'.join(stopwords))  

def ReturnCleanText(text):
         text = text.lower()
         text = re.sub(r"\W+|_", ' ', text)
         return re.sub(stop_pat, '', text)

df2.title = df2.title.apply(ReturnCleanText)

df2tokens=nltk.word_tokenize("/n".join(df2.title))
df2lemma = [wnl.lemmatize(t) for t in df2tokens]

count2 = collections.Counter(df2lemma)
word_freq2 = pd.DataFrame(count2.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)

mask3 = (newsmax['date'] > '2022-02-01') & (newsmax['date'] <= '2022-02-14')
df3 = fox.loc[mask3]
title03 = " ".join(d for d in df3.title.astype(str))

stop_pat = r'\b(?:{})\b'.format('|'.join(stopwords))  

def ReturnCleanText(text):
         text = text.lower()
         text = re.sub(r"\W+|_", ' ', text)
         return re.sub(stop_pat, '', text)

df3.title = df3.title.apply(ReturnCleanText)

df3tokens=nltk.word_tokenize("/n".join(df3.title))
df3lemma = [wnl.lemmatize(t) for t in df3tokens]

count3 = collections.Counter(df3lemma)
word_freq3 = pd.DataFrame(count3.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)

fig, axes = plt.subplots(3,1,figsize=(5,30))
sns.set(font_scale = 1.5)
sns.barplot(ax=axes[0],x='frequency', y='word', data=word_freq1.head(30), palette="Oranges_r").set(title='Early February News Word Frequency for Breitbart News')
sns.barplot(ax=axes[1],x='frequency', y='word', data=word_freq2.head(30), palette="flare_r").set(title='Early February News Word Frequency for Fox News')
sns.barplot(ax=axes[2],x='frequency', y='word', data=word_freq3.head(30), palette="Oranges_r").set(title='Early February News Word Frequency for Newsmax News')
sns.set_style("ticks")

mask = (breitbart['date'] > '2022-03-01') & (breitbart['date'] <= '2022-03-14') # select a period
df1 = breitbart.loc[mask]
title01 = " ".join(d for d in df1.title.astype(str))

stop_pat = r'\b(?:{})\b'.format('|'.join(stopwords))  

def ReturnCleanText(text):
         text = text.lower()
         text = re.sub(r"\W+|_", ' ', text)
         return re.sub(stop_pat, '', text)

df1.title = df1.title.apply(ReturnCleanText)

df1tokens=nltk.word_tokenize("/n".join(df1.title))
df1lemma = [wnl.lemmatize(t) for t in df1tokens]

count1 = collections.Counter(df1lemma)
word_freq1 = pd.DataFrame(count1.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)

mask2 = (fox['date'] > '2022-03-01') & (fox['date'] <= '2022-03-14')
df2 = fox.loc[mask2]
title02 = " ".join(d for d in df2.title.astype(str))

stop_pat = r'\b(?:{})\b'.format('|'.join(stopwords))  

def ReturnCleanText(text):
         text = text.lower()
         text = re.sub(r"\W+|_", ' ', text)
         return re.sub(stop_pat, '', text)

df2.title = df2.title.apply(ReturnCleanText)

df2tokens=nltk.word_tokenize("/n".join(df2.title))
df2lemma = [wnl.lemmatize(t) for t in df2tokens]

count2 = collections.Counter(df2lemma)
word_freq2 = pd.DataFrame(count2.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)

mask3 = (newsmax['date'] > '2022-03-01') & (newsmax['date'] <= '2022-03-14')
df3 = fox.loc[mask3]
title03 = " ".join(d for d in df3.title.astype(str))

stop_pat = r'\b(?:{})\b'.format('|'.join(stopwords))  

def ReturnCleanText(text):
         text = text.lower()
         text = re.sub(r"\W+|_", ' ', text)
         return re.sub(stop_pat, '', text)

df3.title = df3.title.apply(ReturnCleanText)

df3tokens=nltk.word_tokenize("/n".join(df3.title))
df3lemma = [wnl.lemmatize(t) for t in df3tokens]

count3 = collections.Counter(df3lemma)
word_freq3 = pd.DataFrame(count3.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)

fig, axes = plt.subplots(3,1,figsize=(5,30))
sns.set(font_scale = 1.5)
sns.barplot(ax=axes[0],x='frequency', y='word', data=word_freq1.head(30), palette="Oranges_r").set(title='Early March News Word Frequency for Breitbart News')
sns.barplot(ax=axes[1],x='frequency', y='word', data=word_freq2.head(30), palette="flare_r").set(title='Early March News Word Frequency for Fox News')
sns.barplot(ax=axes[2],x='frequency', y='word', data=word_freq3.head(30), palette="Oranges_r").set(title='Early March News Word Frequency for Newsmax News')
sns.set_style("ticks")

tokens=nltk.word_tokenize(textb)

mywords = [w.lower() for w in tokens]
revisedwords = [w for w in mywords if w.isalpha()] #words only

stopwords=nltk.corpus.stopwords.words("english")
tokentext=[w for w in revisedwords if not w in stopwords] #remove all the stop words

def apwords(words):
    filtered_sentence = []
    words = nltk.word_tokenize(words)
    for w in words:
        filtered_sentence.append(w)
    return filtered_sentence
addwords = lambda x: apwords(x)
breitbart['words'] = breitbart['text'].apply(addwords)

def apwords(words):
    filtered_sentence = []
    words = nltk.word_tokenize(words)
    for w in words:
        filtered_sentence.append(w)
    return filtered_sentence
addwords = lambda x: apwords(x)
fox['words'] = fox['text'].apply(addwords)

def apwords(words):
    filtered_sentence = []
    words = nltk.word_tokenize(words)
    for w in words:
        filtered_sentence.append(w)
    return filtered_sentence
addwords = lambda x: apwords(x)
newsmax['words'] = newsmax['text'].apply(addwords)

stopwords = STOPWORDS.update(['say','says','said','saying'])

text01 = " ".join(d for d in breitbart.text.astype(str))

wc = WordCloud(width = 800, height = 800,
                background_color ='white',
                min_font_size = 10,
                stopwords = stopwords)
wc.generate(text01)
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wc,interpolation = 'bilinear')
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

text02 = " ".join(d for d in fox.text.astype(str))

uploaded = files.upload()

from PIL import Image

#mask = np.array(Image.open('rep.png'))
wc = WordCloud(width = 800, height = 800,
                background_color ='white',
                min_font_size = 2,
               stopwords = stopwords,)
               #mask = mask)
wc.generate(text02)
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wc,interpolation = 'bilinear')
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

text03 = " ".join(d for d in newsmax.text.astype(str))

wc = WordCloud(width = 800, height = 800,
                background_color ='white',
                min_font_size = 10,
               stopwords = stopwords)
wc.generate(text03)
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wc,interpolation = 'bilinear')
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer

breitbart['polarity'] = ''
for i,x in breitbart.text.iteritems():
    label = TextBlob(x)
    breitbart['polarity'][i] = label.sentiment.polarity

plt.figure(figsize=(4,8))
sns.set(font_scale = 1.3)
sns.histplot(data=breitbart, y=breitbart.polarity,hue=breitbart.label).set(title='Sentiment Polarity for Breitbart News')
plt.ylim(-0.6,0.6)
plt.show()

def polarity_to_label(x):
    if(x <= 0):
        return 'negative'
    if(x > 0):
        return 'positive'
breitbart.label = breitbart.polarity.apply(polarity_to_label)

breitbart.label.value_counts()

fox['polarity'] = ''
for i,x in fox.text.iteritems():
    label = TextBlob(x)
    fox['polarity'][i] = label.sentiment.polarity

def polarity_to_label(x):
    if(x <= 0):
        return 'negative'
    if(x > 0):
        return 'positive'
fox.label = fox.polarity.apply(polarity_to_label)

plt.figure(figsize=(4,8))
sns.set(font_scale = 1.3)
sns.histplot(data=fox, y=fox.polarity,hue=fox.label).set(title='Sentiment Polarity for Fox News')
plt.ylim(-0.6,0.6)
plt.show()

fox.label.value_counts()

newsmax['polarity'] = ''
for i,x in newsmax.text.iteritems():
    label = TextBlob(x)
    newsmax['polarity'][i] = label.sentiment.polarity

def polarity_to_label(x):
    if(x <= 0):
        return 'negative'
    if(x > 0):
        return 'positive'
newsmax.label = newsmax.polarity.apply(polarity_to_label)

plt.figure(figsize=(4,8))
sns.set(font_scale = 1.3)
sns.histplot(data=newsmax, y=newsmax.polarity,hue=newsmax.label).set(title='Sentiment Polarity for Newsmax')
plt.ylim(-0.6,0.6)
plt.show()

newsmax.label.value_counts()

newsmax.to_csv("newsmax.csv", encoding='utf-8')
files.download('newsmax.csv') #download the csv file

breitbart.to_csv("breitbart.csv", encoding='utf-8')
files.download('breitbart.csv') #download the csv file

fox.to_csv("fox.csv", encoding='utf-8')
files.download('fox.csv') #download the csv file